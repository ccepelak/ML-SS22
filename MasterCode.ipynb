{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-9k7v2it7 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "/.local/lib/python3.8/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as sts\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import random as rd\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import decomposition\n",
    "from sklearn import pipeline\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "#from skimage import io, color\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,  GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pygeos\n",
    "import ntpath\n",
    "import geopandas as gpd\n",
    "#import geoplot\n",
    "from shapely.geometry import Point, Polygon\n",
    "from fiona.crs import from_epsg\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_HOME'] = '/workspace/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES= 0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "rd.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU'),\n",
       " LogicalDevice(name='/device:GPU:1', device_type='GPU'),\n",
       " LogicalDevice(name='/device:GPU:2', device_type='GPU'),\n",
       " LogicalDevice(name='/device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize the data and flatten images if necessary\n",
    "def transform(dataset):\n",
    "    new_list = []\n",
    "    for i in range(len(dataset)):\n",
    "        #resize the image\n",
    "        temp2 = resize(dataset[i], (256, 256), Image.NEAREST)\n",
    "        \n",
    "        #flatten it\n",
    "        new_list.append(temp2.flatten())\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagePreProcess(pathLink, outcomeLink, n_samples = 100, n_comps_filter = 30):\n",
    "    \"\"\"\n",
    "    func: Filteres and preprocesses the images for one country\n",
    "    \n",
    "    input:\n",
    "        -pathLink: path to image files\n",
    "        -outcomeLink: path to outcome file\n",
    "        -n_samples: no. of samples in PCA, default 100\n",
    "        -n_comps_filter: no. of components to filter, default 30 \n",
    "        \n",
    "    \n",
    "    output: \n",
    "        -indices for filtered array\n",
    "        -transformed images\n",
    "        -geoframe with matched images\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Opening files from\", pathLink)\n",
    "    \n",
    "    #get all the pathlinks to the images\n",
    "    raw_images_links = glob(pathLink)\n",
    "    \n",
    "    #open the images\n",
    "    img_arrays = [np.array(Image.open(file_name)) for file_name in raw_images_links]\n",
    "    \n",
    "    #transform images\n",
    "    img_transform = transform(img_arrays)\n",
    "    \n",
    "    print(\"Filtering Images\")\n",
    "    \n",
    "    #filter the images using a PCA\n",
    "    n_samples = 100 #set higher for more accuracy \n",
    "    n_comps_filter = 30 #set higher for more accuracy \n",
    "    \n",
    "    pca_filter = PCA(n_comps_filter)\n",
    "    pca_filter.fit(rd.sample(img_transform, n_samples))\n",
    "    print(f\"Completed training the PCA for filtering with {n_samples} samples and {n_comps_filter} components. \\nvar expl: {round(np.cumsum(pca_filter.explained_variance_ratio_)[n_comps_filter-1]*100)}%\")\n",
    "    img_pca_filter = pca_filter.transform(img_transform)\n",
    "    \n",
    "    #filter out the unique images\n",
    "    img_unique, unique_indices = np.unique(img_pca_filter, axis= 0, return_index= True)\n",
    "    \n",
    "    print(\"Finding Location of images\")\n",
    "    # Get the location of the images \n",
    "    image_geodf = gpd.GeoDataFrame() # Create an empty geopandas GeoDataFrame\n",
    "    for i in unique_indices:\n",
    "        #extract an array of the geo points from the file name \n",
    "        point_array = [float(point) for point in str.split(ntpath.basename(raw_images_links[i][:-4]),\"_\")]\n",
    "\n",
    "        lat_i = 1\n",
    "        long_i = 0\n",
    "        #calculate the width of the image taken\n",
    "        #https://wiki.openstreetmap.org/wiki/Zoom_levels \n",
    "        meters = 40075016.686 * math.cos(math.radians(point_array[lat_i]))/(2**14)\n",
    "\n",
    "        #the tiles are x-meters wide. Let's find that in degrees: \n",
    "        # (https://stackoverflow.com/questions/25237356/convert-meters-to-decimal-degrees)\n",
    "        width_deg_half = (meters / (111.32 * 1000 * math.cos(point_array[lat_i] * (math.pi / 180))))/2\n",
    "\n",
    "        geo_point = [(point_array[lat_i] - width_deg_half, point_array[long_i]+ width_deg_half),\n",
    "                    (point_array[lat_i] + width_deg_half, point_array[long_i]+ width_deg_half),\n",
    "                    (point_array[lat_i] + width_deg_half, point_array[long_i] - width_deg_half),\n",
    "                    (point_array[lat_i] - width_deg_half, point_array[long_i] - width_deg_half)]\n",
    "\n",
    "        # Create a Shapely polygon from the coordinate-tuple list\n",
    "        image_geodf.loc[i, 'geometry'] = Polygon(geo_point)\n",
    "        # assign an image id. used for merging with actual images and the survey values\n",
    "        image_geodf.loc[i, 'image_id'] = i  \n",
    "\n",
    "        # Set the GeoDataFrame's coordinate system to WGS84 (i.e. epsg code 4326)\n",
    "    image_geodf.crs = from_epsg(4326)\n",
    "    \n",
    "    print(\"Loading Outcome\")\n",
    "    \n",
    "    #load outcome data\n",
    "    outcome_geodf = gpd.read_file(outcomeLink)\n",
    "    \n",
    "    \n",
    "    geo_df = gpd.sjoin(image_geodf, outcome_geodf, how=\"inner\", predicate = \"contains\")\n",
    "    \n",
    "    ## ASSEMBLE DATASET FOR COUNTRY\n",
    "    X_country = []\n",
    "    Y_country = []\n",
    "    for i in range(len(geo_df)):\n",
    "        obs = geo_df.iloc[i]\n",
    "        X_country.append(img_transform[int(obs['image_id'])])\n",
    "        Y_country.append(obs['wealth'])\n",
    "    \n",
    "    return X_country, Y_country\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files from image_data/malawi_archive/images/*\n",
      "Filtering Images\n",
      "Completed training the PCA for filtering with 100 samples and 30 components. \n",
      "var expl: 89%\n"
     ]
    }
   ],
   "source": [
    "X_mala, Y_mala =  imagePreProcess(\"image_data/malawi_archive/images/*\", \n",
    "                                  \"outcome_data/mlw_gdf.shp\")\n",
    "\n",
    "X_mali, Y_mali = imagePreProcess(\"image_data/Mali_archive/images/*\", \n",
    "                                \"outcome_data/mli_gdf.shp\")\n",
    "\n",
    "X_ni, Y_ni = imagePreProcess(\"image_data/nigeria_archive/images/*\", \n",
    "                                \"outcome_data/ngr_gdf.shp\")\n",
    "\n",
    "X_eth, Y_eth = imagePreProcess(\"image_data/ethiopia_archive/images/ethiopia_Images/*\", \n",
    "                                \"outcome_data/eth_gdf.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpd.read(\"outcome_data/mlw_gdf.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_mala))\n",
    "print(len(X_mali))\n",
    "print(len(X_ni))\n",
    "print(len(X_eth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of X dataframes\n",
    "X_full = X_mala + X_mali + X_eth + X_ni\n",
    "\n",
    "# List of Y dataframes\n",
    "Y_full = Y_mala + Y_mali + Y_eth + Y_ni\n",
    "\n",
    "print(len(X_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, Y_full, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find how many components we should use by just applying a PCA with a range of components\n",
    "\n",
    "#initialize the pca\n",
    "pca = PCA(whiten=True, copy=True, random_state= 10)\n",
    "#fit it\n",
    "pca.fit(rd.sample(X_train,100))\n",
    "\n",
    "#get the cumulative \n",
    "total = np.cumsum(pca.explained_variance_ratio_)\n",
    "total[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "comp_to_view = 25\n",
    "#let's plot it\n",
    "plot_xs = list(range(comp_to_view))\n",
    "\n",
    "\n",
    "plt.scatter(plot_xs, pca.explained_variance_ratio_[:comp_to_view], color='black', label='Explained variance proportion')\n",
    "plt.scatter(plot_xs, total[:comp_to_view],color='orange', label='Total')\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.xlabel('Number of Component', size=16)\n",
    "plt.ylabel('Variance Explained (%)', size=16)\n",
    "#plt.hlines(0.,xmin=0, xmax=comp_to_view, colors = 'blue', linestyles='--')\n",
    "plt.grid('minor')\n",
    "plt.show()\n",
    "\n",
    "print(pca.explained_variance_ratio_[:comp_to_view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the pca\n",
    "\n",
    "n_comp = 25\n",
    "pca = PCA(n_components = n_comp, whiten=True, copy=True, random_state= 10)\n",
    "\n",
    "#fit with random sample of _ observations\n",
    "pca.fit(rd.sample(X_train,10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## batchwise transformation of the \n",
    "\n",
    "reduced = []\n",
    "batchsize = 2500\n",
    "# \n",
    "for i in range(0, len(X_train), batchsize):\n",
    "    if i == 0:\n",
    "        reduced = pca.transform(X_train[i:i+batchsize])\n",
    "        continue \n",
    "    \n",
    "    reduced_batch = pca.transform(X_train[i:i+batchsize])\n",
    "    reduced = np.concatenate((reduced, reduced_batch), axis =0)\n",
    "\n",
    "X_train_pca = np.asarray(reduced)\n",
    "n_obs = len(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "#function straight copied from\n",
    "def plot_gallery(title, images, n_col, n_row, cmap_p=plt.cm.gray):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        #print(vmax)\n",
    "        plt.imshow((comp*255).astype(np.uint8),\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax \n",
    "                   #cmap = cmap_p\n",
    "                   )\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenimages = pca.components_.reshape(n_comp,256,256,4).copy()\n",
    "plot_gallery(\"PCA\", eigenimages[:,:,:,:3], n_col=5, n_row=5, cmap_p = plt.cm.viridis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.score(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(X_train_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((y_train))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators= 100, bootstrap = True)\n",
    "rf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Grid Search on Random Forest Regressor\n",
    "\n",
    "Source and documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "Tuning: \n",
    "- n_estimators: number of trees in the forest\n",
    "- max_depth: maximum depth of the tree \n",
    "- max_features: number of features taken into account when splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_n_list = [100]\n",
    "hyper_depth_list = [1,2,3,4,5, 10,16,18,20,22,24,26, 30, 40, 50, 100, 200]\n",
    "hyper_m_list = [\"auto\", \"sqrt\", \"log2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=0),\n",
    "                           {\n",
    "                              'n_estimators':hyper_n_list,\n",
    "                              'max_depth': hyper_depth_list,\n",
    "                              'max_features': hyper_m_list,\n",
    "                            },cv=4, scoring=\"r2\",verbose=1,n_jobs=-1\n",
    "                           )\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_plot2d(df, param, outcome, group_var = None):\n",
    "\n",
    "    '''\n",
    "    func: function to plot the performance of a parameter\n",
    "    - df (dataframe): cross-validation grid search dataframe\n",
    "\n",
    "    params have to be column names in DF\n",
    "    - param (str): which parameter to asses\n",
    "    - outcome (str): which outcome to find the max for\n",
    "    - group_var (str): grouping variable (default None)\n",
    "    '''\n",
    "\n",
    "    group_var_list = np.unique(np.array(df[group_var]))\n",
    "    plt.figure(figsize=(12,8))\n",
    "    #print(np.argmax(np.asarray(df[outcome])))\n",
    "    best_score = np.max(np.asarray(df[outcome]))\n",
    "    best_score_param = np.asarray(df[param])[np.argmax(np.asarray(df[outcome]))]\n",
    "    if group_var != None:\n",
    "        best_score_group = np.asarray(df[group_var])[np.argmax(np.asarray(df[outcome]))]\n",
    "        #print(best_score_group)\n",
    "\n",
    "    if group_var == None:\n",
    "        plt.plot(param,outcome,data = df)\n",
    "        plt.xlabel(param)\n",
    "        plt.ylabel(outcome)\n",
    "        #plt.grid()\n",
    "        plt.vlines(best_score_param, 0, np.max(np.asarray(df[outcome])+.05))\n",
    "        plt.text(best_score_param,best_score - (best_score*.1),  f'Best parameter: {best_score_param} with score: {best_score}',size =12)\n",
    "        plt.legend()\n",
    "    else:\n",
    "        for inst in group_var_list:\n",
    "            plt.plot(param,outcome,label = inst, data = df[df[group_var]== inst])\n",
    "            plt.xlabel(param)\n",
    "            plt.ylabel(outcome)\n",
    "            #plt.grid()\n",
    "            plt.text(best_score_param,best_score - (best_score*.1),  f'Best parameter: {best_score_param} \\n with score: {round(best_score,3)} \\nin group:{best_score_group}', size = 12)\n",
    "            plt.vlines(best_score_param, 0, best_score+.05)\n",
    "            plt.legend(loc = 4)\n",
    "            plt.ylim(bottom =0 )\n",
    "    plt.grid()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_plot2d(cv_df, 'param_max_depth', 'mean_test_score', 'param_max_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_plot2d(cv_df, 'param_max_features' ,'mean_test_score','param_max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import gc\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from tensorflow.keras import applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = np.asarray(X_train).reshape(n_obs, 256, 256, 4)\n",
    "#X_train_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = X_train_cnn[:,:,:,:3].astype('float32') # Taking out the fourth channel as ghe information is the same everywhere\n",
    "y_train_cnn = np.asarray(y_train).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1_cnn, X_val_cnn, y_train1_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train_cnn, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/\n",
    "from keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_satellite = Sequential()\n",
    "cnn_satellite.add(Conv2D(32, kernel_size=(16, 16),activation='linear',input_shape=(256,256,3),padding='same'))\n",
    "cnn_satellite.add(LeakyReLU(alpha=0.1))\n",
    "cnn_satellite.add(MaxPooling2D((2, 2),padding='same'))\n",
    "cnn_satellite.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "cnn_satellite.add(LeakyReLU(alpha=0.1))\n",
    "cnn_satellite.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "cnn_satellite.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "cnn_satellite.add(LeakyReLU(alpha=0.1))                  \n",
    "cnn_satellite.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "cnn_satellite.add(Flatten())\n",
    "cnn_satellite.add(Dense(32, activation='linear'))\n",
    "cnn_satellite.add(LeakyReLU(alpha=0.1))                  \n",
    "cnn_satellite.add(Dense(1, activation='linear'))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_satellite.compile(loss=keras.losses.mean_squared_error, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=[coeff_determination])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_satellite.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "#os.environ['TORCH_HOME'] = '/workspace/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "954"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    cnn_satellite = Sequential()\n",
    "    cnn_satellite.add(Conv2D(32, kernel_size=(16, 16),activation='linear',input_shape=(256,256,3),padding='same'))\n",
    "    cnn_satellite.add(LeakyReLU(alpha=0.1))\n",
    "    cnn_satellite.add(MaxPooling2D((2, 2),padding='same'))\n",
    "    cnn_satellite.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "    cnn_satellite.add(LeakyReLU(alpha=0.1))\n",
    "    cnn_satellite.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    cnn_satellite.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "    cnn_satellite.add(LeakyReLU(alpha=0.1))                  \n",
    "    cnn_satellite.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    cnn_satellite.add(Flatten())\n",
    "    cnn_satellite.add(Dense(32, activation='linear'))\n",
    "    cnn_satellite.add(LeakyReLU(alpha=0.1))                  \n",
    "    cnn_satellite.add(Dense(1, activation='linear'))\n",
    "    cnn_satellite.compile(loss=keras.losses.mean_squared_error, \n",
    "                      optimizer=keras.optimizers.Adam(),\n",
    "                      metrics=[coeff_determination])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cnn_res50 = applications.resnet_v2.preprocess_input(\n",
    "    X_train1_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "epochs = 2\n",
    "batchsize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and record the history \n",
    "cnn_history = cnn_satellite.fit(X_cnn_res50, \n",
    "                  y_train1_cnn, \n",
    "                  batch_size=batchsize, \n",
    "                  epochs=epochs, \n",
    "                  verbose=1\n",
    "                  #validation_data=(X_val_cnn, y_val_cnn)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = cnn_satellite.evaluate(X_val_cnn, y_val_cnn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(epochs)], cnn_history.history['val_coeff_determination'], color = 'grey')\n",
    "plt.scatter([i for i in range(epochs)], cnn_history.history['val_coeff_determination'])\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation $R^2$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
